{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34a490",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib widget\n",
    "%pdb off\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from IPython.display import display, HTML\n",
    "import flowEmulationUtils as feUtils\n",
    "import random\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "display(HTML(\n",
    "    '<script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG\"></script>'\n",
    "))\n",
    "\n",
    "#close all figures\n",
    "plt.close('all')\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "im_scaling = .75\n",
    "plt.rcParams['figure.figsize'] = [6.4 * im_scaling, 4.8 * im_scaling]\n",
    "\n",
    "home_dir = \"./\"\n",
    "display(home_dir)\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8c8c0",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c3a57",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "multiRun_dir = f\"{home_dir}/CHARLES/multiRuns/\"\n",
    "plotFolder = f\"{multiRun_dir}\"\n",
    "\n",
    "roomVentilationMI = pd.read_csv(f\"{multiRun_dir}/roomVentilationMIEmulation.csv\", index_col = [0,1])\n",
    "flowStatsMI = pd.read_csv(f\"{multiRun_dir}/flowStatsMIEmulation.csv\", index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Dev/Test Assignment\n",
    "# Create a column called split and assign 70 % to train, 10% to dev, and 20% to test in roomVentilationMI\n",
    "random.seed(42)  # For reproducibility\n",
    "roomVentilationMI[\"split\"] = roomVentilationMI.index.to_series().apply(lambda _: random.random())\n",
    "roomVentilationMI[\"split\"] = roomVentilationMI[\"split\"].apply(lambda x: \"train\" if x < 0.7 else (\"dev\" if x < 0.8 else \"test\"))\n",
    "\n",
    "for (run, room), row in roomVentilationMI.iterrows():\n",
    "    windowKeyCols = roomVentilationMI.columns[\n",
    "        roomVentilationMI.columns.str.contains(\"windowKeys\")\n",
    "    ].tolist()\n",
    "    windowKeys = row[windowKeyCols].dropna()\n",
    "    for windowKey in windowKeys:\n",
    "        flowStatsMI.loc[(run, windowKey), \"split\"] = row[\"split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = flowStatsMI.copy()\n",
    "# df = df[df[\"slAll\"] == False]\n",
    "# normalize x_cols. Flow quantities to be normalized by WS. Pressures to be normalized by W**2:\n",
    "\n",
    "p_norm_cols = []\n",
    "u_norm_cols = []\n",
    "no_norm_cols = []\n",
    "for col in df.columns:\n",
    "    if \"-p0\" in col or \"p_\" in col or \"(p)\" in col or \"p0meas\" in col or \"u**2\" in col:\n",
    "        p_norm_cols.append(col)\n",
    "    elif \"mag\" in col or \"shear\" in col or \"normal\" in col or \"flux\" in col or \"(u\" in col or \"q_model\" in col:\n",
    "        u_norm_cols.append(col)\n",
    "    else:\n",
    "        no_norm_cols.append(col)\n",
    "\n",
    "print(f\"Normalizing p cols: {sorted(p_norm_cols)}\")\n",
    "print(f\"Normalizing u cols: {sorted(u_norm_cols)}\")\n",
    "print(f\"Not normalizing cols: {sorted(no_norm_cols)}\")\n",
    "\n",
    "# Normalize pressure columns by WS^2\n",
    "df[p_norm_cols] = df[p_norm_cols].div(df[\"WS\"]**2, axis=0)\n",
    "# Normalize velocity columns by WS\n",
    "df[u_norm_cols] = df[u_norm_cols].div(df[\"WS\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77739484",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if \"orientation\" in col:\n",
    "        sin_col = col.replace(\"orientation\", \"sin\")\n",
    "        cos_col = col.replace(\"orientation\", \"cos\")\n",
    "        df[sin_col] = np.sin(df[col])\n",
    "        df[cos_col] = np.cos(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d91d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['p-noInt_optp0-q_modelC_d'] = df['p-noInt_optp0-q_model'] * df['p-noInt_optp0-C_d']\n",
    "# for col in df.columns:\n",
    "#     if \"mag\" in col or \"shear\" in col or \"normal\" in col:\n",
    "#         df[col] = df[col] / df['p-noInt_optp0-q_modelC_d']\n",
    "\n",
    "df[\"skylight\"] = df['openingType'].apply(lambda x: 1 if \"skylight\" in x else 0)\n",
    "df[\"cross\"] = df['openingType'].apply(lambda x: 1 if \"cross\" in x else 0)\n",
    "df[\"single\"] = df['openingType'].apply(lambda x: 1 if \"single\" in x else 0)\n",
    "df[\"dual\"] = df['openingType'].apply(lambda x: 1 if \"dual\" in x else 0)\n",
    "df[\"corner\"] = df['openingType'].apply(lambda x: 1 if \"corner\" in x else 0)\n",
    "Sdelp = np.sign(df['p-noInt_optp0-q_model'])\n",
    "Sdelp[Sdelp == 0] = 1  # Assign 1 to zero values\n",
    "df[\"Sdelp\"] = Sdelp\n",
    "df[\"EP_shear-noInt-qIn\"] = df[\"EP_shear-noInt\"] * df[\"Sdelp\"] > 0\n",
    "df[\"EP_shear-noInt-qOut\"] = df[\"EP_shear-noInt\"] * df[\"Sdelp\"] <= 0\n",
    "\n",
    "df[\"all\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ydf = df['p-noInt-C_d'].copy()\n",
    "ydf = df['p-noInt_optp0Cd-C_d'].copy()\n",
    "# ydf = df['flux'].copy()\n",
    "\n",
    "\n",
    "# x_cols = df.columns.values\n",
    "# x_cols = [col for col in x_cols if (\"noInt\" in col)]# or \"EP_\" in col)]\n",
    "# x_cols = [col for col in x_cols if (\"x-\" not in col and \"y-\" not in col and \"z-\" not in col)]\n",
    "# x_cols = [col for col in x_cols if (\"_x\" not in col and \"_y\" not in col and \"_z\" not in col)]\n",
    "# x_cols = [col for col in x_cols if \"run\" not in col]\n",
    "# x_cols = [col for col in x_cols if \"q-\" not in col]\n",
    "# x_cols = [col for col in x_cols if \"comp(u_avg,0)\" not in col]\n",
    "# x_cols = [col for col in x_cols if \"comp(u_avg,2)\" not in col]\n",
    "# x_cols += [\"skylight\", \"AofA\", \"sinAofA\", \"cosAofA\", \"slAll\", \"delT\", \"SS\"]\n",
    "# # x_cols += [\"mean-mass_flux\", \"mean-sn_prod(abs(u))\"]\n",
    "# x_cols = [col for col in x_cols if \"C_d\" not in col]\n",
    "# x_cols = [col for col in x_cols if \"mass_flux\" not in col]\n",
    "# x_cols = [col for col in x_cols if \"sn_prod(abs(u))\" not in col]\n",
    "# x_cols = [col for col in x_cols if \"q_model\" not in col]\n",
    "# x_cols += [\"Sdelp\", \"all\", \"Ri\", \"WS\"]\n",
    "# x_cols += ['p-noInt_optp0-C_d', 'p-noInt_optp0-q_model']\n",
    "# x_cols += ['cross', 'single', 'dual', 'corner'] \n",
    "\n",
    "# x_cols = [ \"skylight\", \"delT\", \"SS\", \"p-noInt_optp0-p0\", \"p_avg-noInt\", \"p-noInt_optp0-q_model\", \"EP_normal-noInt\", \"EP_shear-noInt\", \"EP_vel_orientation-noInt\", \"EPR_vel_orientation-noInt\", \"p_rms-noInt\", \"EPR_mag-noInt\", \"AofA\", \"Sdelp\", \"all\"]\n",
    "# x_cols = [\"p-noInt_optp0-q_model\", \"skylight\", \"delT\", \"SS\", \"p-noInt_optp0-p0\", \"p_avg-noInt\", \"EP_normal-noInt\", \"EP_shear-noInt\", \"EP_vel_orientation-noInt\", \"p_rms-noInt\", \"Sdelp\", \"all\", \"EP_shear-noInt-qIn\",\"EP_shear-noInt-qOut\"]\n",
    "x_cols = [\"p-noInt_optp0-q_model\"]#, \"Sdelp\", \"skylight\", \"delT\", \"SS\", \"EP_vel_sin\", \"EP_vel_cos\", \"EP_vel_orientation\"]#, \"EP_shear-noInt-qIn\",\"EP_shear-noInt-qOut\"]\n",
    "\n",
    "x_cols += [\"split\"]\n",
    "xdf = df[x_cols].copy()\n",
    "\n",
    "display(xdf.columns.values)\n",
    "\n",
    "# ydf = np.log(ydf + .0001)\n",
    "# x_df = np.log(xdf + .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d40a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xdf = xdf.map(lambda s: abs(s) if isinstance(s, (int, float)) else s)  # Ensure all numeric values are positive\n",
    "# ydf = np.abs(ydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = {}\n",
    "y_data = {}\n",
    "# first_level_col = \"roomType\"\n",
    "first_level_col = \"Sdelp\"  # Use Sdelp to group data by sign of p-noInt_optp0-q_model\n",
    "second_level_col = \"skylight\"\n",
    "for first_level in df[first_level_col].unique():\n",
    "    for second_level in df[second_level_col].unique():\n",
    "        rows = (df[first_level_col] == first_level) & (df[second_level_col] == second_level)\n",
    "        if rows.sum() > 0:\n",
    "            x_data[(first_level, second_level)] = xdf[rows]\n",
    "            y_data[(first_level, second_level)] = ydf[rows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ad044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recombine_grouped_data(grouped_data):\n",
    "    df = pd.DataFrame()\n",
    "    for (first_level, second_level), data in grouped_data.items():\n",
    "        if df.empty:\n",
    "            df = data.copy()\n",
    "        else:\n",
    "            df = pd.concat([df, data], axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ffc3d",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Split each group into train/dev/test sets and apply normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263df58",
   "metadata": {},
   "source": [
    "- [ ] Add min max scalar for normalization [-1 1]\n",
    "- [ ] Roughly same distributions -> plot input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def project_to_logspace(x):\n",
    "    \"\"\"\n",
    "    Projects data from [-1, 1] to a range such that log(output) will be in [0, 1].\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Input data in the range [-1, 1]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y : array-like\n",
    "        Transformed data where log(y) will be in the range [-1, 1]\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    e_plus = np.exp(1)   # e^1 ≈ 2.718\n",
    "    e_minus = np.exp(0) # e^0 ≈ 1\n",
    "    \n",
    "    # Linear mapping from [-1, 1] to [e^0, e^1]\n",
    "    # This ensures that log(y) will be in [0, 1]\n",
    "    y = (e_plus - e_minus) / 2 * (x + 1) + e_minus\n",
    "    \n",
    "    return np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dictionaries to store our preprocessed data\n",
    "x_train_data = {}\n",
    "x_dev_data = {}\n",
    "x_test_data = {}\n",
    "y_train_data = {}\n",
    "y_dev_data = {}\n",
    "y_test_data = {}\n",
    "\n",
    "x_train_data_unscaled = {}\n",
    "x_dev_data_unscaled = {}\n",
    "x_test_data_unscaled = {}\n",
    "y_train_data_unscaled = {}\n",
    "y_dev_data_unscaled = {}\n",
    "y_test_data_unscaled = {}\n",
    "\n",
    "# Dictionaries for transformers\n",
    "column_transformers = {}\n",
    "y_scalers = {}\n",
    "\n",
    "    \n",
    "# 2. Identify column groups by name\n",
    "ori_cols = [c for c in xdf.columns\n",
    "            if 'orientation' in c.lower()\n",
    "            or c == 'AofA']\n",
    "trig_cols = [c for c in xdf.columns\n",
    "            if 'sin' in c.lower()\n",
    "            or 'cos' in c.lower()]\n",
    "other_cols = [c for c in xdf.columns\n",
    "            if c not in ori_cols + trig_cols + [\"split\"]]\n",
    "\n",
    "# Process each group\n",
    "for group, X in x_data.items():\n",
    "    first_level, second_level = group\n",
    "    y = y_data[group]\n",
    "    \n",
    "    print(f\"Processing {first_level} - {second_level} group...\")\n",
    "    \n",
    "    train_mask = X['split'] == 'train'\n",
    "    dev_mask = X['split'] == 'dev'\n",
    "    test_mask = X['split'] == 'test'\n",
    "    X.drop(columns=['split'], inplace=True)  # Remove split column from features\n",
    "    \n",
    "    # 1. Split data into train/dev/test\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_dev,   y_dev   = X[dev_mask],   y[dev_mask]\n",
    "    X_test,  y_test  = X[test_mask],  y[test_mask]\n",
    "\n",
    "    # Stor the split data\n",
    "    x_train_data_unscaled[group] = X_train.copy()\n",
    "    x_dev_data_unscaled[group] = X_dev.copy()\n",
    "    x_test_data_unscaled[group] = X_test.copy()\n",
    "    y_train_data_unscaled[group] = y_train.copy()\n",
    "    y_dev_data_unscaled[group] = y_dev.copy()\n",
    "    y_test_data_unscaled[group] = y_test.copy()\n",
    "\n",
    "    \n",
    "    # 3. Build a ColumnTransformer\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            ('A_scale',\n",
    "            FunctionTransformer(func=lambda x: (x - 180)/180,\n",
    "                                inverse_func=lambda x: x*180 + 180,\n",
    "                                validate=False),\n",
    "            ori_cols),\n",
    "            ('passthrough_trig','passthrough',trig_cols),\n",
    "            ('scale', MinMaxScaler(feature_range=(-1,1)), other_cols),\n",
    "        ],\n",
    "        remainder='drop')  # drop any unexpected columns\n",
    "    \n",
    "    # 4. Fit on TRAIN, transform all splits\n",
    "    X_train_s = ct.fit_transform(X_train)\n",
    "    X_dev_s = ct.transform(X_dev)\n",
    "    X_test_s = ct.transform(X_test)\n",
    "\n",
    "    # #4.1 Optionally project to log space\n",
    "    # X_train_s = project_to_logspace(X_train_s)\n",
    "    # X_dev_s = project_to_logspace(X_dev_s)\n",
    "    # X_test_s = project_to_logspace(X_test_s)\n",
    "    \n",
    "    # 5. Manually fit + apply y‐scaler on TRAIN only\n",
    "    scaler_y = MinMaxScaler().fit(y_train.values.reshape(-1,1))\n",
    "    \n",
    "    y_train_s = scaler_y.transform(y_train.values.reshape(-1,1)).ravel()\n",
    "    y_dev_s = scaler_y.transform(y_dev.values.reshape(-1,1)).ravel()\n",
    "    y_test_s = scaler_y.transform(y_test.values.reshape(-1,1)).ravel()\n",
    "\n",
    "    # #5.1 Optionally project to log space\n",
    "    # y_train_s = project_to_logspace(y_train_s)\n",
    "    # y_dev_s = project_to_logspace(y_dev_s)\n",
    "    # y_test_s = project_to_logspace(y_test_s)\n",
    "    \n",
    "    # 6. (Optionally wrap back to DataFrame for convenience:)\n",
    "    out_cols = ori_cols + trig_cols + other_cols\n",
    "    X_train_s = pd.DataFrame(X_train_s, columns=out_cols, index=X_train.index)\n",
    "    X_dev_s = pd.DataFrame(X_dev_s, columns=out_cols, index=X_dev.index)\n",
    "    X_test_s = pd.DataFrame(X_test_s, columns=out_cols, index=X_test.index)\n",
    "    \n",
    "    # Store preprocessed data\n",
    "    x_train_data[group] = X_train_s\n",
    "    x_dev_data[group] = X_dev_s\n",
    "    x_test_data[group] = X_test_s\n",
    "    y_train_data[group] = y_train_s\n",
    "    y_dev_data[group] = y_dev_s\n",
    "    y_test_data[group] = y_test_s\n",
    "    \n",
    "    # Store transformers for later inverse transformations\n",
    "    column_transformers[group] = ct\n",
    "    y_scalers[group] = scaler_y\n",
    "    \n",
    "    print(f\"  Train: {X_train_s.shape[0]} samples\")\n",
    "    print(f\"  Dev:   {X_dev_s.shape[0]} samples\")\n",
    "    print(f\"  Test:  {X_test_s.shape[0]} samples\")\n",
    "    print()\n",
    "\n",
    "# Print summary\n",
    "print(f\"Processed {len(x_train_data)} groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5db103",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Combine all training data into one DataFrame\n",
    "# X_train_combined = pd.concat(list(x_train_data.values()), axis=0)\n",
    "# # Create a DataFrame with training targets\n",
    "# y_train_combined = np.concatenate(list(y_train_data.values()), axis=0)\n",
    "\n",
    "# # Perform SVD on the training feature matrix\n",
    "# n_components = min(20, len(X_train_combined.columns))\n",
    "# svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "# X_svd = svd.fit_transform(X_train_combined)\n",
    "\n",
    "# # Plot explained variance ratio\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(np.cumsum(svd.explained_variance_ratio_), marker='o')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.title('Explained Variance by SVD Components (Training Data)')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Get feature importance based on SVD components\n",
    "# feature_importance = np.abs(svd.components_).sum(axis=0)\n",
    "# feature_importance = feature_importance / feature_importance.sum()  # Normalize\n",
    "\n",
    "# # Create DataFrame with feature names and importance scores\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'Feature': X_train_combined.columns,\n",
    "#     'Importance': feature_importance\n",
    "# })\n",
    "# importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# # Plot top 15 features by importance\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "# plt.title('Top 15 Features by SVD Importance (Training Data)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Weight feature importance by explained variance\n",
    "# weighted_importance = np.zeros(svd.components_.shape[1])\n",
    "# for i, var_ratio in enumerate(svd.explained_variance_ratio_):\n",
    "#     weighted_importance += np.abs(svd.components_[i]) * var_ratio\n",
    "\n",
    "# # Normalize\n",
    "# weighted_importance = weighted_importance / weighted_importance.sum()\n",
    "\n",
    "# # Create DataFrame with feature names and weighted importance scores\n",
    "# weighted_importance_df = pd.DataFrame({\n",
    "#     'Feature': X_train_combined.columns,\n",
    "#     'Weighted_Importance': weighted_importance\n",
    "# })\n",
    "# weighted_importance_df = weighted_importance_df.sort_values('Weighted_Importance', ascending=False)\n",
    "\n",
    "# # Plot top 15 features by weighted importance\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Weighted_Importance', y='Feature', data=weighted_importance_df.head(15))\n",
    "# plt.title('Top Features by Variance-Weighted Importance (Training Data)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Look at the first 3 principal components and their feature contributions\n",
    "# n_top_features = 10\n",
    "# plt.figure(figsize=(15, 12))\n",
    "\n",
    "# for i in range(min(3, n_components)):\n",
    "#     plt.subplot(3, 1, i+1)\n",
    "#     component = pd.Series(svd.components_[i], index=X_train_combined.columns)\n",
    "#     component_df = pd.DataFrame({\n",
    "#         'Feature': component.index,\n",
    "#         'Loading': component.values\n",
    "#     })\n",
    "#     component_df = component_df.reindex(component_df.Loading.abs().sort_values(ascending=False).index)\n",
    "    \n",
    "#     # Plot top contributing features\n",
    "#     sns.barplot(x='Loading', y='Feature', data=component_df.head(n_top_features))\n",
    "#     plt.title(f'Top {n_top_features} Features in Component {i+1} (Explained Variance: {svd.explained_variance_ratio_[i]:.2%})')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Create a heatmap of the top features' correlation matrix\n",
    "# top_features = importance_df.head(30)['Feature'].values\n",
    "# correlation_matrix = X_train_combined[top_features].corr()\n",
    "\n",
    "# plt.figure(figsize=(14, 12))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "# plt.title('Correlation Matrix of Top Features (Training Data)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Check how the first few SVD components correlate with the target variable\n",
    "# svd_df = pd.DataFrame(X_svd, columns=[f'SVD_{i+1}' for i in range(n_components)])\n",
    "# svd_df['target'] = y_train_combined\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# for i in range(min(5, n_components)):\n",
    "#     plt.subplot(2, 3, i+1)\n",
    "#     plt.scatter(svd_df[f'SVD_{i+1}'], svd_df['target'], alpha=0.5)\n",
    "#     plt.xlabel(f'SVD Component {i+1}')\n",
    "#     plt.ylabel('y')\n",
    "#     plt.title(f'Target vs SVD Component {i+1}')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"SVD analysis completed on training data. Top 5 most important features:\")\n",
    "# display(importance_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9080385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "\n",
    "# # Create scatter plots of the target variable against the top 5 features from SVD analysis\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Get the top 5 features from the SVD importance analysis\n",
    "# top_features = importance_df.head(11)['Feature'].values\n",
    "# print(f\"Top 11 features by importance: {top_features}\")\n",
    "\n",
    "# # Create a figure with subplots for each feature\n",
    "# fig, axes = plt.subplots(4, 3, figsize=(18, 12))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # For storing correlation values\n",
    "# correlations = {}\n",
    "\n",
    "# # Plot each feature against the target\n",
    "# for i, feature in enumerate(top_features):\n",
    "#     ax = axes[i]\n",
    "    \n",
    "#     # Calculate correlation and p-value\n",
    "#     corr, p_value = stats.pearsonr(X_train_combined[feature], y_train_combined)\n",
    "#     correlations[feature] = (corr, p_value)\n",
    "    \n",
    "#     # Create scatter plot with regression line\n",
    "#     sns.regplot(x=X_train_combined[feature], y=y_train_combined, ax=ax, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})\n",
    "    \n",
    "#     # Add correlation information\n",
    "#     ax.text(0.05, 0.95, f\"Correlation: {corr:.3f}\\np-value: {p_value:.3e}\", \n",
    "#             transform=ax.transAxes, bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "#     ax.set_title(f\"y vs {feature}\")\n",
    "#     ax.set_xlabel(feature)\n",
    "#     ax.set_ylabel(\"y\")\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "\n",
    "# # Create a violin plot in the last subplot showing the distribution of the target\n",
    "# sns.violinplot(y=y_train_combined, ax=axes[11], inner=\"quartile\")\n",
    "# axes[11].set_title(\"Distribution of y\")\n",
    "# axes[11].set_ylabel(\"y\")\n",
    "# axes[11].grid(True, alpha=0.3)\n",
    "\n",
    "# # Add a horizontal line at y=0 for reference\n",
    "# axes[5].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Display correlation summary\n",
    "# print(\"\\nCorrelation summary:\")\n",
    "# for feature, (corr, p_value) in sorted(correlations.items(), key=lambda x: abs(x[1][0]), reverse=True):\n",
    "#     sig = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "#     print(f\"{feature}: r = {corr:.3f} {sig} (p = {p_value:.3e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Visualize feature distributions after normalization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot feature distributions\n",
    "def plot_feature_distributions(data_dict, feature_groups=None, n_cols=3, figsize=(18, 15)):\n",
    "    \"\"\"\n",
    "    Plot distributions of features across different groups.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        Dictionary mapping (first_level, second_level) to DataFrames containing the features\n",
    "    feature_groups : dict, optional\n",
    "        Dictionary mapping group names to lists of feature names\n",
    "    n_cols : int, optional\n",
    "        Number of columns in the grid\n",
    "    figsize : tuple, optional\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    # If no feature groups provided, create a default one with all features\n",
    "    if feature_groups is None:\n",
    "        # Get a sample dataframe to extract all column names\n",
    "        sample_df = next(iter(data_dict.values()))\n",
    "        feature_groups = {'All Features': sample_df.columns.tolist()}\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # For each feature group\n",
    "    for group_idx, (group_name, features) in enumerate(feature_groups.items()):\n",
    "        print(f\"\\n--- {group_name} ---\")\n",
    "        \n",
    "        # Calculate how many rows we need\n",
    "        n_rows = (len(features) + n_cols - 1) // n_cols\n",
    "        \n",
    "        # Create subplot grid\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "        \n",
    "        # For each feature in this group\n",
    "        for i, feature in enumerate(features):\n",
    "            ax = fig.add_subplot(gs[i // n_cols, i % n_cols])\n",
    "            \n",
    "            # Collect all values for this feature across all groups\n",
    "            all_values = []\n",
    "            for group, df in data_dict.items():\n",
    "                if feature in df.columns:\n",
    "                    all_values.extend(df[feature].values)\n",
    "            \n",
    "            # Plot histogram\n",
    "            sns.histplot(all_values, kde=True, ax=ax)\n",
    "            ax.set_title(f\"{feature}\")\n",
    "            ax.set_xlabel(\"\")\n",
    "            \n",
    "            # Calculate and display statistics\n",
    "            mean_val = np.mean(all_values)\n",
    "            std_val = np.std(all_values)\n",
    "            min_val = np.min(all_values)\n",
    "            max_val = np.max(all_values)\n",
    "            \n",
    "            stats_text = f\"Mean: {mean_val:.2f}\\nStd: {std_val:.2f}\\nMin: {min_val:.2f}\\nMax: {max_val:.2f}\"\n",
    "            ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, \n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "            \n",
    "            # Print statistics\n",
    "            print(f\"{feature}: Mean={mean_val:.2f}, Std={std_val:.2f}, Min={min_val:.2f}, Max={max_val:.2f}\")\n",
    "        \n",
    "        plt.suptitle(f\"Distribution of {group_name}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "# Organize features by groups\n",
    "feature_groups = {\n",
    "    'Orientation Features': ori_cols,\n",
    "    'Trigonometric Features': trig_cols,\n",
    "    'Other Features': other_cols[:10],  # First 10 other features\n",
    "    'More Other Features': other_cols[10:20],  # Next 10 other features\n",
    "    'Remaining Features': other_cols[20:]  # Remaining features\n",
    "}\n",
    "\n",
    "# Plot distributions for training data\n",
    "# plot_feature_distributions(x_train_data, feature_groups)\n",
    "\n",
    "# Compare distributions across different room and opening types\n",
    "for feature in ['EP_mag-noInt', 'EP_shear-noInt', 'EP_normal-noInt', 'sinAofA', 'cosAofA']:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for group, dfplot in x_train_data.items():\n",
    "        if feature in dfplot.columns:\n",
    "            sns.kdeplot(dfplot[feature].values, label=f\"{group[0]}-{group[1]}\")\n",
    "    plt.title(f\"Distribution of {feature} across different room and opening types\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b703e28",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f1523",
   "metadata": {},
   "source": [
    "Rob Suggestions\n",
    "- Gaussian process (sklearn)\n",
    "- linear regression\n",
    "- bin into room types\n",
    "- window flag\n",
    "- process window and room types separately at first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717c63f",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6290ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_linear_likelihood(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate log likelihood for a linear regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Input features\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    log_likelihood : float\n",
    "        Log likelihood of the data under the model\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Estimate variance (MLE of variance)\n",
    "    n = len(y)\n",
    "    variance = np.sum(residuals**2) / n\n",
    "    \n",
    "    # Calculate average log likelihood\n",
    "    log_likelihood = np.mean(stats.norm.logpdf(y, loc=y_pred, scale=np.sqrt(variance)))\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "def calculate_normalized_rmse(y, y_pred, normalization='std'):\n",
    "    \"\"\"\n",
    "    Calculate RMSE and normalized RMSE for any regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Input features\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values\n",
    "    normalization : str, optional (default='std')\n",
    "        Method for normalization:\n",
    "        - 'std': normalize by standard deviation of y\n",
    "        - 'mean': normalize by mean of y\n",
    "        - 'range': normalize by range of y\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    rmse : float\n",
    "        Root Mean Square Error\n",
    "    nrmse : float\n",
    "        Normalized Root Mean Square Error\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    mse = np.mean((y - y_pred)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate normalized RMSE\n",
    "    if normalization == 'mean':\n",
    "        # Normalize by mean of observed values\n",
    "        nrmse = rmse / np.mean(np.abs(y))\n",
    "    elif normalization == 'range':\n",
    "        # Normalize by range of observed values\n",
    "        nrmse = rmse / (np.max(y) - np.min(y))\n",
    "    else:  # default: 'std'\n",
    "        # Normalize by standard deviation of observed values\n",
    "        nrmse = rmse / np.std(y)\n",
    "    \n",
    "    return rmse, nrmse\n",
    "\n",
    "def visualize_linear_model(linear_model, X, y, y_pred=None, hue=None, style=None, model_name=\"Linear Regression\", feature_names=None, top_features=10, y_transformer=None):\n",
    "    \"\"\"\n",
    "    Create a comprehensive visualization of linear model fit with feature importance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    linear_model : sklearn linear model (LinearRegression, Ridge, Lasso, etc.)\n",
    "        The fitted linear model\n",
    "    X : DataFrame\n",
    "        Input features as pandas DataFrame\n",
    "    y : Series or array\n",
    "        Target values\n",
    "    model_name : str, optional\n",
    "        Name of the model type for plot titles\n",
    "    feature_names : list, optional\n",
    "        Names of features (if not provided, will use X.columns)\n",
    "    top_features : int, optional\n",
    "        Number of top features to show in importance plot\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Convert y to numpy array if it's a pandas Series\n",
    "    if hasattr(y, 'values'):\n",
    "        y_values = y.values\n",
    "    else:\n",
    "        y_values = np.array(y)\n",
    "\n",
    "    if hue is not None and hasattr(X, 'index'):\n",
    "        hue = hue.loc[X.index]\n",
    "    if style is not None and hasattr(X, 'index'):\n",
    "        style = style.loc[X.index]\n",
    "    \n",
    "    # Get feature names from DataFrame if not provided\n",
    "    if feature_names is None and hasattr(X, 'columns'):\n",
    "        feature_names = X.columns.tolist()\n",
    "    elif feature_names is None:\n",
    "        feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    # Get predictions\n",
    "    if y_pred is None:\n",
    "        y_pred = linear_model.predict(X)\n",
    "\n",
    "    if y_transformer is not None:\n",
    "        # Inverse transform predictions if a transformer is provided\n",
    "        y_pred = y_transformer.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "        y_values = y_transformer.inverse_transform(y_values.reshape(-1, 1)).ravel()\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_values, y_pred)\n",
    "    rmse, nrmse = calculate_normalized_rmse(y_values, y_pred)\n",
    "    log_likelihood = calculate_linear_likelihood(y_values, y_pred)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_values - y_pred\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Actual vs Prediction plot (swapped axes)\n",
    "    sns.scatterplot(x=y_pred, y=y_values, hue=hue, style=style, alpha=0.6, ax=axes[0, 0])\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(min(y_values), min(y_pred))\n",
    "    max_val = max(max(y_values), max(y_pred))\n",
    "    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Predicted Values')\n",
    "    axes[0, 0].set_ylabel('Actual Values')\n",
    "    axes[0, 0].set_title(f'{model_name}: Actual vs Prediction\\nR² = {r2:.4f}, NRMSE = {nrmse:.4f}, RMSE = {rmse:.4f}, Log Likelihood = {log_likelihood:.4f}')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Residuals vs Actual\n",
    "    sns.scatterplot(x=y_values, y=residuals, hue=hue, style=style, alpha=0.6, ax=axes[0, 1])\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Actual Values')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residuals vs Actual')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Distribution\n",
    "    sns.histplot(residuals, kde=True, ax=axes[1, 0])\n",
    "    axes[1, 0].axvline(x=0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Residual Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Residual Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. QQ Plot for Residuals\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot of Residuals')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature Importance from Coefficients\n",
    "    if hasattr(linear_model, 'coef_'):\n",
    "        # Get coefficients\n",
    "        if linear_model.coef_.ndim > 1:\n",
    "            coeffs = linear_model.coef_[0]  # For multi-output models\n",
    "        else:\n",
    "            coeffs = linear_model.coef_\n",
    "        \n",
    "        # Create DataFrame with features and coefficients\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Coefficient': coeffs\n",
    "        })\n",
    "        \n",
    "        # Add absolute coefficient for ranking\n",
    "        coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])\n",
    "        \n",
    "        # Sort by absolute coefficient\n",
    "        coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "        \n",
    "        # Show top features\n",
    "        top_coef = coef_df.head(top_features)\n",
    "        \n",
    "        # Plot coefficients\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        colors = ['red' if x < 0 else 'blue' for x in top_coef['Coefficient']]\n",
    "        bars = sns.barplot(x='Coefficient', y='Feature', data=top_coef, palette=colors)\n",
    "        \n",
    "        # Add a vertical line at x=0\n",
    "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Add value labels to the bars\n",
    "        for i, v in enumerate(top_coef['Coefficient']):\n",
    "            bars.text(v + (0.01 if v >= 0 else -0.01), i, f\"{v:.4f}\", \n",
    "                     va='center', ha='left' if v >= 0 else 'right')\n",
    "        \n",
    "        plt.title(f'Top {top_features} Feature Coefficients in {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display coefficient table\n",
    "        print(\"\\nFeature Coefficient Ranking:\")\n",
    "        display(coef_df)\n",
    "        \n",
    "        # Feature correlation with target\n",
    "        if hasattr(X, 'corrwith'):\n",
    "            print(\"\\nFeature Correlation with Target:\")\n",
    "            corr_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Correlation': X.corrwith(pd.Series(y_values, index=X.index)).values\n",
    "            })\n",
    "            corr_df['Abs_Correlation'] = np.abs(corr_df['Correlation'])\n",
    "            corr_df = corr_df.sort_values('Abs_Correlation', ascending=False)\n",
    "            display(corr_df)\n",
    "            \n",
    "            # Plot correlation heatmap for top features\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            top_features_list = coef_df.head(min(15, len(feature_names)))['Feature'].tolist()\n",
    "            X_top = X[top_features_list]\n",
    "            \n",
    "            # Add target to the correlation matrix\n",
    "            X_with_y = X_top.copy()\n",
    "            X_with_y['Target'] = y_values\n",
    "            \n",
    "            # Plot correlation heatmap\n",
    "            sns.heatmap(X_with_y.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "            plt.title('Correlation Heatmap of Top Features with Target ({model_name})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "lr_results = {}\n",
    "\n",
    "for group in x_train_data.keys():\n",
    "    first_level, second_level = group\n",
    "    X_train = x_train_data[group]#[[\"p-noInt_optp0-q_model\"]]\n",
    "    y_train = y_train_data[group]\n",
    "    X_dev = x_dev_data[group]#[[\"p-noInt_optp0-q_model\"]]\n",
    "    y_dev = y_dev_data[group]\n",
    "    \n",
    "    # Fit multivariable linear model (sklearn)\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Collect coefficients per feature\n",
    "    coeffs = dict(zip(X_train.columns, model.coef_))\n",
    "    intercept = model.intercept_\n",
    "    r2_train = model.score(X_train, y_train)\n",
    "    r2_dev = model.score(X_dev, y_dev)\n",
    "    LL_train = calculate_linear_likelihood(y_train, model.predict(X_train))\n",
    "    LL_dev = calculate_linear_likelihood(y_dev, model.predict(X_dev))\n",
    "    rmse_train, nrmse_train = calculate_normalized_rmse(y_train, model.predict(X_train))\n",
    "    rmse_dev, nrmse_dev = calculate_normalized_rmse(y_dev, model.predict(X_dev))\n",
    "    \n",
    "    # Significance testing (statsmodels OLS)\n",
    "    X_sm = sm.add_constant(X_train)\n",
    "    results_sm = sm.OLS(y_train, X_sm).fit()\n",
    "    pvalues = results_sm.pvalues.to_dict()\n",
    "    \n",
    "    # Store results\n",
    "    lr_results[group] = {\n",
    "        'coefficients': coeffs,\n",
    "        'intercept': intercept,\n",
    "        'r2_train': r2_train,\n",
    "        'r2_dev': r2_dev,\n",
    "        'LL_train': LL_train,\n",
    "        'LL_dev': LL_dev,\n",
    "        'nrmse_train': rmse_train,\n",
    "        'nrmse_dev': rmse_dev,\n",
    "        'pvalues': pvalues,\n",
    "        'model': model,\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"{first_level} - {second_level}: R²(train)={r2_train:.3f}, R²(dev)={r2_dev:.3f}, LL(train)={LL_train:.3f}, LL(dev)={LL_dev:.3f}, nrmse(train)={nrmse_train:.3f}, nrmse(dev)={nrmse_dev:.3f}, rmse(train)={rmse_train:.3f}, rmse(dev)={rmse_dev:.3f}\")\n",
    "    for feat, coef in coeffs.items():\n",
    "        print(f\"  {feat}: {coef:.3f}\")\n",
    "    print(f\"  intercept: {intercept:.3f}\")\n",
    "    # Print p-values with significance marker\n",
    "    print(\"  p-values:\")\n",
    "    for var, pval in pvalues.items():\n",
    "        sig = '*' if pval < 0.05 else ''\n",
    "        print(f\"    {var}: {pval:.3f}{sig}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc50f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of significant p-values across all (roomType, openingType) groups\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "sig_counts = Counter()\n",
    "sig_keys = defaultdict(list)\n",
    "for (first_level, second_level), res in lr_results.items():\n",
    "    # Skip intercept\n",
    "    for var, pval in res['pvalues'].items():\n",
    "        if var == 'const':\n",
    "            continue\n",
    "        if pval < 0.05:\n",
    "            sig_counts[var] += 1\n",
    "            sig_keys[var].append((first_level, second_level))\n",
    "\n",
    "# Display results\n",
    "print(\"Significant counts per variable (p < 0.05)):\")\n",
    "for var, count in sig_counts.items():\n",
    "    groups = sig_keys[var]\n",
    "    print(f\"{var}: {count} (groups: {groups})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of variables ranked by p-value for each group\n",
    "import pandas as pd\n",
    "\n",
    "ranked = {}\n",
    "for key, res in lr_results.items():\n",
    "    # Build Series of p-values, exclude intercept\n",
    "    pvals = pd.Series(res['pvalues'])\n",
    "    pvals = pvals.drop('const', errors='ignore')\n",
    "    # Sort by p-value ascending and take variable names\n",
    "    ranked[key] = pvals.sort_values().index.tolist()\n",
    "\n",
    "# Construct DataFrame; rows = rank order, columns = groups\n",
    "ranked_df = pd.DataFrame(ranked)\n",
    "display(ranked_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f114c",
   "metadata": {},
   "source": [
    "## Regularized Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06927111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression for each (roomType, openingType) group\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_results = {}\n",
    "# Set regularization strength (alpha); increase for more shrinkage\n",
    "alpha = 1\n",
    "\n",
    "for group in x_train_data.keys():\n",
    "    first_level, second_level = group\n",
    "    X_train = x_train_data[group]\n",
    "    y_train = y_train_data[group] \n",
    "    X_dev = x_dev_data[group]\n",
    "    y_dev = y_dev_data[group]\n",
    "    \n",
    "    # Fit Ridge model\n",
    "    ridge = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Collect coefficients and intercept\n",
    "    coeffs = dict(zip(X_train.columns, ridge.coef_))\n",
    "    intercept = ridge.intercept_\n",
    "    r2_train = ridge.score(X_train, y_train)\n",
    "    r2_dev = ridge.score(X_dev, y_dev)\n",
    "    LL_train = calculate_linear_likelihood(y_train, ridge.predict(X_train))\n",
    "    LL_dev = calculate_linear_likelihood(y_dev, ridge.predict(X_dev))\n",
    "    rmse_train, nrmse_train = calculate_normalized_rmse(y_train, ridge.predict(X_train))\n",
    "    rmse_dev, nrmse_dev = calculate_normalized_rmse(y_dev, ridge.predict(X_dev))\n",
    "    \n",
    "    ridge_results[group] = {\n",
    "        'coefficients': coeffs, \n",
    "        'intercept': intercept, \n",
    "        'r2_train': r2_train,\n",
    "        'r2_dev': r2_dev,\n",
    "        'LL_train': LL_train,\n",
    "        'LL_dev': LL_dev,\n",
    "        'nrmse_train': rmse_train,\n",
    "        'nrmse_dev': nrmse_dev,\n",
    "        'model': ridge\n",
    "    }\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Ridge α={alpha} | {first_level}-{second_level}: R²(train)={r2_train:.3f}, R²(dev)={r2_dev:.3f}, LL(train)={LL_train:.3f}, LL(dev)={LL_dev:.3f}, nrmse(train)={nrmse_train:.3f}, nrmse(dev)={nrmse_dev:.3f}\")\n",
    "    for feat, coef in coeffs.items():\n",
    "        print(f\"  {feat}: {coef:.3f}\")\n",
    "    print(f\"  intercept: {intercept:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression for each (roomType, openingType) group\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_results = {}\n",
    "# Set L1 regularization strength; increase alpha for more sparsity\n",
    "alpha_lasso = 0.01\n",
    "\n",
    "for group in x_train_data.keys():\n",
    "    first_level, second_level = group\n",
    "    X_train = x_train_data[group]\n",
    "    y_train = y_train_data[group]\n",
    "    X_dev = x_dev_data[group]\n",
    "    y_dev = y_dev_data[group]\n",
    "    \n",
    "    # Fit Lasso model\n",
    "    lasso = Lasso(alpha=alpha_lasso, max_iter=10000, fit_intercept=True)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Collect coefficients and intercept\n",
    "    coeffs = dict(zip(X_train.columns, lasso.coef_))\n",
    "    intercept = lasso.intercept_\n",
    "    r2_train = lasso.score(X_train, y_train)\n",
    "    r2_dev = lasso.score(X_dev, y_dev)\n",
    "    LL_train = calculate_linear_likelihood(y_train, lasso.predict(X_train))\n",
    "    LL_dev = calculate_linear_likelihood(y_dev, lasso.predict(X_dev))\n",
    "    rmse_train, nrmse_train = calculate_normalized_rmse(y_train, lasso.predict(X_train))\n",
    "    rmse_dev, nrmse_dev = calculate_normalized_rmse(y_dev, lasso.predict(X_dev))\n",
    "    \n",
    "    lasso_results[group] = {\n",
    "        'coefficients': coeffs, \n",
    "        'intercept': intercept, \n",
    "        'r2_train': r2_train,\n",
    "        'r2_dev': r2_dev,\n",
    "        'LL_train': LL_train,\n",
    "        'LL_dev': LL_dev,\n",
    "        'nrmse_train': rmse_train,\n",
    "        'nrmse_dev': nrmse_dev,\n",
    "        'model': lasso\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Lasso α={alpha_lasso} | {first_level}-{second_level}: R²(train)={r2_train:.3f}, R²(dev)={r2_dev:.3f}, LL(train)={LL_train:.3f}, LL(dev)={LL_dev:.3f}, nrmse(train)={nrmse_train:.3f}, nrmse(dev)={nrmse_dev:.3f}\")\n",
    "    for feat, coef in coeffs.items():\n",
    "        print(f\"  {feat}: {coef:.3f}\")\n",
    "    print(f\"  intercept: {intercept:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Lasso coefficient sparsity across groups\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nonzero_counts = Counter()\n",
    "nonzero_keys = defaultdict(list)\n",
    "for (first_level, second_level), res in lasso_results.items():\n",
    "    for var, coef in res['coefficients'].items():\n",
    "        if abs(coef) > 1e-6:  # Use a small threshold to account for floating point\n",
    "            nonzero_counts[var] += 1\n",
    "            nonzero_keys[var].append((first_level, second_level))\n",
    "\n",
    "# Display results\n",
    "print(f\"Non-zero coefficient counts per variable (Lasso α={alpha_lasso}):\")\n",
    "for var, count in sorted(nonzero_counts.items(), key=lambda x: -x[1]):\n",
    "    groups = nonzero_keys[var]\n",
    "    print(f\"{var}: {count} (groups: {groups})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2325d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQFromCd(results, x_data, split, y_scaler=None):\n",
    "    y_preds = []\n",
    "    for group, res in results.items():\n",
    "        X = x_data[group]\n",
    "        model = res['model']\n",
    "        y_pred = model.predict(X)\n",
    "        y_transformer = y_scalers[group]\n",
    "        y_pred = y_transformer.inverse_transform(y_pred.reshape(-1, 1)).ravel()  # Inverse transform predictions\n",
    "        y_preds.append(y_pred)\n",
    "    y_pred = np.concatenate(y_preds, axis=0)\n",
    "\n",
    "    dfq_pred = flowStatsMI[flowStatsMI[\"split\"] == split].copy()\n",
    "    dfq_pred[\"C_d\"] = y_pred\n",
    "    roomVentilationMI_pred = roomVentilationMI[roomVentilationMI[\"split\"] == split].copy()\n",
    "\n",
    "    dfq_pred, roomVentilationMI_pred = feUtils.update_flow_and_ventilation(dfq_pred, roomVentilationMI_pred, optTypes = [\"optp0\"])\n",
    "    y_pred = dfq_pred[\"p-noInt_optp0-q_model\"]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linear regression model\n",
    "# group = ('corner', 'xwindow_0-0')  # Choose a group\n",
    "# group = ('cross', 'zwindow')  # Choose a group\n",
    "group = (True, True)  # Choose a group\n",
    "X_train = x_train_data[group]\n",
    "y_train = y_train_data[group]\n",
    "X_dev = x_dev_data[group]\n",
    "y_dev = y_dev_data[group]\n",
    "y_transformer = y_scalers[group]\n",
    "\n",
    "\n",
    "visualize_linear_model(lr_results[group][\"model\"], X_train, y_train, y_pred=None, style=df['openingType'], hue=df['roomType'], model_name=\"Linear Regression\", y_transformer=y_transformer)\n",
    "# visualize_linear_model(ridge_results[group][\"model\"], X_train, y_train, y_pred=None, style=df['openingType'], hue=df['roomType'], model_name=\"Ridge Regression\", y_transformer=y_transformer)\n",
    "# visualize_linear_model(lasso_results[group][\"model\"], X_train, y_train, y_pred=None, style=df['openingType'], hue=df['roomType'], model_name=\"Lasso Regression\", y_transformer=y_transformer)\n",
    "\n",
    "\n",
    "y_ref = flowStatsMI.loc[X_train.index, \"flux\"]\n",
    "\n",
    "y_pred = getQFromCd(lr_results, x_train_data, \"train\", y_scalers)\n",
    "y_pred = y_pred.loc[X_train.index]  # split out indexes\n",
    "visualize_linear_model(lr_results[group][\"model\"], X_train, y_ref, y_pred=y_pred, style=df['openingType'], hue=df['roomType'], model_name=\"Linear Regression\", y_transformer=None)\n",
    "\n",
    "# y_pred = getQFromCd(ridge_results, x_train_data, \"train\", y_scalers)\n",
    "# y_pred = y_pred.loc[X_train.index]  # split out indexes\n",
    "# visualize_linear_model(ridge_results[group][\"model\"], X_train, y_ref, y_pred=y_pred, style=df['openingType'], hue=df['roomType'], model_name=\"Ridge Regression\", y_transformer=None)\n",
    "\n",
    "# y_pred = getQFromCd(lasso_results, x_train_data, \"train\", y_scalers)\n",
    "# y_pred = y_pred.loc[X_train.index]  # split out indexes\n",
    "# visualize_linear_model(lasso_results[group][\"model\"], X_train, y_ref, y_pred=y_pred, style=df['openingType'], hue=df['roomType'], model_name=\"Lasso Regression\", y_transformer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "lr_results_unscaled = {}\n",
    "y_pred_train = {}\n",
    "y_pred_dev = {}\n",
    "y_pred_test = {}\n",
    "\n",
    "for group in x_train_data.keys():\n",
    "    print(group)\n",
    "    first_level, second_level = group\n",
    "    X_train = x_train_data_unscaled[group]#[[\"p-noInt_optp0-q_model\"]]\n",
    "    y_train = y_train_data_unscaled[group]\n",
    "    X_dev = x_dev_data_unscaled[group]#[[\"p-noInt_optp0-q_model\"]]\n",
    "    y_dev = y_dev_data_unscaled[group]\n",
    "    X_test = x_test_data_unscaled[group]\n",
    "    y_test = y_test_data_unscaled[group]\n",
    "    \n",
    "    # Fit multivariable linear model (sklearn)\n",
    "    model = LinearRegression(fit_intercept=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Collect coefficients per feature\n",
    "    coeffs = dict(zip(X_train.columns, model.coef_))\n",
    "    intercept = model.intercept_\n",
    "    r2_train = model.score(X_train, y_train)\n",
    "    r2_dev = model.score(X_dev, y_dev)\n",
    "    LL_train = calculate_linear_likelihood(y_train, model.predict(X_train))\n",
    "    LL_dev = calculate_linear_likelihood(y_dev, model.predict(X_dev))\n",
    "    rmse_train, nrmse_train = calculate_normalized_rmse(y_train, model.predict(X_train))\n",
    "    rmse_dev, nrmse_dev = calculate_normalized_rmse(y_dev, model.predict(X_dev))\n",
    "    \n",
    "    # Significance testing (statsmodels OLS)\n",
    "    X_sm = sm.add_constant(X_train)\n",
    "    results_sm = sm.OLS(y_train, X_sm).fit()\n",
    "    pvalues = results_sm.pvalues.to_dict()\n",
    "    \n",
    "    # Store results\n",
    "    lr_results_unscaled[group] = {\n",
    "        'coefficients': coeffs,\n",
    "        'intercept': intercept,\n",
    "        'r2_train': r2_train,\n",
    "        'r2_dev': r2_dev,\n",
    "        'LL_train': LL_train,\n",
    "        'LL_dev': LL_dev,\n",
    "        'nrmse_train': rmse_train,\n",
    "        'nrmse_dev': rmse_dev,\n",
    "        'pvalues': pvalues,\n",
    "        'model': model,\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"{first_level} - {second_level}: R²(train)={r2_train:.3f}, R²(dev)={r2_dev:.3f}, LL(train)={LL_train:.3f}, LL(dev)={LL_dev:.3f}, nrmse(train)={nrmse_train:.3f}, nrmse(dev)={nrmse_dev:.3f}, rmse(train)={rmse_train:.3f}, rmse(dev)={rmse_dev:.3f}\")\n",
    "    for feat, coef in coeffs.items():\n",
    "        print(f\"  {feat}: {coef:.3f}\")\n",
    "    print(f\"  intercept: {intercept:.3f}\")\n",
    "    # Print p-values with significance marker\n",
    "    print(\"  p-values:\")\n",
    "    for var, pval in pvalues.items():\n",
    "        sig = '*' if pval < 0.05 else ''\n",
    "        print(f\"    {var}: {pval:.3f}{sig}\")\n",
    "    print()\n",
    "    \n",
    "    y_pred_train[group] = pd.Series(model.predict(X_train), index=X_train.index)\n",
    "    y_pred_dev[group] = pd.Series(model.predict(X_dev), index=X_dev.index)\n",
    "    y_pred_test[group] = pd.Series(model.predict(X_test), index=X_test.index)\n",
    "\n",
    "X_train = recombine_grouped_data(x_train_data_unscaled)\n",
    "X_dev = recombine_grouped_data(x_dev_data_unscaled)\n",
    "y_train = recombine_grouped_data(y_train_data_unscaled)\n",
    "y_dev = recombine_grouped_data(y_dev_data_unscaled)\n",
    "\n",
    "# if fitting for C_d, get predictions from the model\n",
    "y_pred_train_all = getQFromCd(lr_results_unscaled, x_train_data_unscaled, \"train\")\n",
    "y_pred_train_all = y_pred_train_all.loc[X_train.index]  # split out indexes\n",
    "y_pred_dev_all = getQFromCd(lr_results_unscaled, x_dev_data_unscaled, \"dev\")\n",
    "y_pred_dev_all = y_pred_dev_all.loc[X_dev.index]\n",
    "\n",
    "y_ref = flowStatsMI.loc[X_train.index, \"flux\"]\n",
    "visualize_linear_model(model, X_train, y_ref, y_pred=y_pred_train_all, style=df['openingType'], hue=df['AofA'], model_name=\"Linear Regression (Unscaled)\", top_features=10)\n",
    "y_ref = flowStatsMI.loc[X_dev.index, \"flux\"]\n",
    "visualize_linear_model(model, X_dev, y_ref, y_pred=y_pred_dev_all, style=df['openingType'], hue=df['roomType'], model_name=\"Linear Regression (Unscaled)\", top_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WV_train = df[df[\"split\"] == \"train\"].copy()\n",
    "# y_pred_train_all = recombine_grouped_data(y_pred_train)\n",
    "y_pred_train_all = getQFromCd(lr_results_unscaled, x_train_data_unscaled, \"train\")\n",
    "y_pred_train_all = y_pred_train_all.loc[WV_train.index]  # split out indexes\n",
    "\n",
    "WV_train[\"q-model\"] = y_pred_train_all\n",
    "WV_train[\"q-model\"] = WV_train[\"q-model\"] / WV_train[\"WS\"]\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(24, 12), dpi=140, sharex=True, sharey=True)\n",
    "axs = axs.flatten()  # Flatten for easier indexing\n",
    "Ri_values = sorted(WV_train[\"Ri\"].unique())\n",
    "\n",
    "# Define combinations of Ri, slAll\n",
    "combinations = [\n",
    "    (Ri_values[2],  True), \n",
    "    (Ri_values[1],  True), \n",
    "    (Ri_values[0],  True), \n",
    "    (Ri_values[2],  False),\n",
    "    (Ri_values[1],  False),\n",
    "    (Ri_values[0],  False),\n",
    "]\n",
    "\n",
    "\n",
    "# Value to normalize by\n",
    "rho = 1.225\n",
    "\n",
    "# Create plots for each combination\n",
    "for i, (ri_val, sl_val) in enumerate(combinations):\n",
    "    # Filter data for this combination\n",
    "    plotdf = WV_train.copy()\n",
    "    plotdf = plotdf[np.isclose(plotdf[\"Ri\"], ri_val)]\n",
    "    plotdf = plotdf[plotdf[\"skylight\"] == sl_val]\n",
    "    \n",
    "    # Create the box plot for this subplot\n",
    "    x_var = \"q-model\"\n",
    "    y_var = \"flux\"\n",
    "    sns.scatterplot(data=plotdf, x=x_var, y=y_var, hue=\"roomType\", style=\"slAll\", alpha=0.6, ax=axs[i])\n",
    "    # add 1:1 regression line\n",
    "    min_val = min(plotdf[x_var].min(), plotdf[y_var].min())\n",
    "    max_val = max(plotdf[x_var].max(), plotdf[y_var].max())\n",
    "    axs[i].plot([min_val, max_val], [min_val, max_val], 'r--', label='1:1 Line')\n",
    "    \n",
    "    # Customize the subplot\n",
    "    axs[i].set_title(f\"Ri={ri_val:.4f}, {'Skylight' if sl_val else 'Window'}\", fontsize=14)\n",
    "    axs[i].set_xlabel(x_var if i >= 5 else \"\", fontsize=12)\n",
    "    axs[i].set_ylabel(y_var if i % 5 == 0 else \"\", fontsize=12)\n",
    "    \n",
    "    # Set legend with custom labels\n",
    "    if i == 0:  # Only add detailed legend to first subplot\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(title=\"QOI\", loc='upper right')\n",
    "    else:\n",
    "        axs[i].get_legend().remove()  # Remove redundant legends\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(\"Ventilation Metrics by Room Type, Richardson Number, Steady State, and Skylights\", fontsize=16, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle\n",
    "\n",
    "rmse, nrmse  = calculate_normalized_rmse(WV_train[\"flux\"], WV_train[\"q-model\"], normalization='std')\n",
    "print(f\"Overall NRMSE: {nrmse:.4f}, RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d22162",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_all = recombine_grouped_data(y_pred_train)\n",
    "y_pred_dev_all = recombine_grouped_data(y_pred_dev)\n",
    "y_pred_test_all = recombine_grouped_data(y_pred_test)\n",
    "\n",
    "y_pred = pd.concat([y_pred_train_all, y_pred_dev_all, y_pred_test_all], axis=0)\n",
    "\n",
    "roomVentilationMI[\"q-model-Norm\"] = None\n",
    "for (run, room), row in roomVentilationMI.iterrows():\n",
    "    windowKeyCols = roomVentilationMI.columns[\n",
    "        roomVentilationMI.columns.str.contains(\"windowKeys\")\n",
    "    ].tolist()\n",
    "    windowKeys = row[windowKeyCols].dropna()\n",
    "    q_pred = 0\n",
    "    for windowKey in windowKeys:\n",
    "        q_pred += np.abs(y_pred.loc[[(run, windowKey)]].values[0]) / 2\n",
    "    roomVentilationMI.loc[(run, room), \"q-model-Norm\"] = q_pred\n",
    "\n",
    "roomVentilationMI[\"flux-Norm\"] = roomVentilationMI[\"flux\"] / roomVentilationMI[\"WS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(24, 12), dpi=140)\n",
    "axs = axs.flatten()  # Flatten for easier indexing\n",
    "Ri_values = sorted(RV_train[\"Ri\"].unique())\n",
    "\n",
    "# Define combinations of Ri, SS, and slAll\n",
    "combinations = [\n",
    "    (Ri_values[2], False, True), \n",
    "    (Ri_values[1], False, True), \n",
    "    (Ri_values[2], True,  True), \n",
    "    (Ri_values[1], True,  True), \n",
    "    (Ri_values[0], True,  True), \n",
    "    (Ri_values[2], False, False),\n",
    "    (Ri_values[1], False, False),\n",
    "    (Ri_values[2], True,  False),\n",
    "    (Ri_values[1], True,  False),\n",
    "    (Ri_values[0], True,  False),\n",
    "]\n",
    "\n",
    "\n",
    "# Value to normalize by\n",
    "rho = 1.225\n",
    "\n",
    "# Create plots for each combination\n",
    "for i, (ri_val, ss_val, sl_val) in enumerate(combinations):\n",
    "    # Filter data for this combination\n",
    "    plotdf = RV_train.copy()\n",
    "    plotdf = plotdf[np.isclose(plotdf[\"Ri\"], ri_val)]\n",
    "    plotdf = plotdf[plotdf[\"SS\"] == ss_val]\n",
    "    plotdf = plotdf[plotdf[\"slAll\"] == sl_val]\n",
    "    \n",
    "    # Create the box plot for this subplot\n",
    "    x_var = \"q-model-Norm\"\n",
    "    y_var = \"flux-Norm\"\n",
    "    sns.scatterplot(data=plotdf, x=x_var, y=y_var, hue=\"roomType\", style=\"houseType\", alpha=0.6, ax=axs[i])\n",
    "    \n",
    "    # Customize the subplot\n",
    "    axs[i].set_title(f\"Ri={ri_val:.4f}, {'Steady State' if ss_val else 'Non-Steady State'}, {'With Skylights' if sl_val else 'No Skylights'}\", fontsize=14)\n",
    "    axs[i].set_xlabel(x_var if i >= 5 else \"\", fontsize=12)\n",
    "    axs[i].set_ylabel(y_var if i % 5 == 0 else \"\", fontsize=12)\n",
    "    \n",
    "    # Set legend with custom labels\n",
    "    if i == 0:  # Only add detailed legend to first subplot\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(title=\"QOI\", loc='upper right')\n",
    "    else:\n",
    "        axs[i].get_legend().remove()  # Remove redundant legends\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(\"Ventilation Metrics by Room Type, Richardson Number, Steady State, and Skylights\", fontsize=16, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 1.225\n",
    "RV_train = roomVentilationMI[roomVentilationMI[\"split\"] == \"train\"]\n",
    "RV_train[\"mean-mass_flux(S)-Norm\"] /= -rho\n",
    "RV_train[\"mean-mass_flux-Norm\"] /= rho\n",
    "RV_train[\"q-D-room-Norm\"] /= rho\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(data=RV_train, x=\"q-model-Norm\", y=\"flux-Norm\", hue=\"AofA\", style=\"roomType\", alpha=0.6)\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(data=RV_train, x=\"q-model-Norm\", y=\"q-D-room-Norm\", hue=\"SS\", style=\"Ri\", alpha=0.6)\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(data=RV_train, x=\"flux-Norm\", y=\"q-D-room-Norm\", hue=\"SS\", style=\"Ri\", alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d67a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ri_values = sorted(RV_train[\"Ri\"].unique())\n",
    "\n",
    "# Value to normalize by\n",
    "rho = 1.225\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(24, 12), dpi=140)\n",
    "axs = axs.flatten()  # Flatten for easier indexing\n",
    "for i in range(len(axs)):\n",
    "    # Filter data for this combination\n",
    "    plotdf = RV_train.copy()\n",
    "    plotdf = plotdf[plotdf[\"slAll\"] == bool(i)]\n",
    "    \n",
    "    # Create the box plot for this subplot\n",
    "    sns.scatterplot(data=plotdf, x=\"q-model-Norm\", y=\"q-D-room-Norm\", hue=\"Ri\", style=\"SS\", alpha=0.6, ax=axs[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91126d",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43831db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel, DotProduct\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gpr_likelihood(gpr_model, X=None, y=None, use_kernel_noise=True):\n",
    "    \"\"\"\n",
    "    Calculate log likelihood for a Gaussian Process Regressor using kernel variance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gpr_model : GaussianProcessRegressor\n",
    "        The fitted GPR model\n",
    "    X : array-like, shape (n_samples, n_features), optional\n",
    "        Input features. If None, assumes we want training set likelihood.\n",
    "    y : array-like, shape (n_samples,), optional\n",
    "        Target values. If None, assumes we want training set likelihood.\n",
    "    use_kernel_noise : bool, default=True\n",
    "        Whether to use the noise variance from the kernel (if available)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    log_likelihood : float\n",
    "        Log likelihood of the data under the model\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Case 1: No data provided - return training log marginal likelihood\n",
    "    if X is None or y is None:\n",
    "        if hasattr(gpr_model, 'log_marginal_likelihood_value_'):\n",
    "            return gpr_model.log_marginal_likelihood_value_ / len(gpr_model.y_train_)  # Normalize by number of data points\n",
    "        else:\n",
    "            return gpr_model.log_marginal_likelihood(gpr_model.kernel_.theta) / len(gpr_model.y_train_)\n",
    "    \n",
    "    # Case 2: Data provided - calculate likelihood for this dataset\n",
    "    y_mean, y_std = gpr_model.predict(X, return_std=True)\n",
    "    \n",
    "    # Try to extract noise variance from kernel if WhiteKernel is used\n",
    "    noise_variance = None\n",
    "    if use_kernel_noise:\n",
    "        try:\n",
    "            # Attempt to extract noise variance from kernel\n",
    "            if hasattr(gpr_model.kernel_, 'k2') and 'WhiteKernel' in str(gpr_model.kernel_.k2):\n",
    "                noise_variance = gpr_model.kernel_.k2.noise_level\n",
    "            elif hasattr(gpr_model.kernel_, 'noise_level'):\n",
    "                noise_variance = gpr_model.kernel_.noise_level\n",
    "            elif hasattr(gpr_model, 'alpha'):\n",
    "                # Alpha in GPR is often used as the noise variance\n",
    "                noise_variance = gpr_model.alpha\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # If we found a noise variance from the kernel, use it\n",
    "    if noise_variance is not None:\n",
    "        # Combine predictive variance with noise variance\n",
    "        total_variance = y_std**2 + noise_variance\n",
    "        std_dev = np.sqrt(total_variance)\n",
    "    else:\n",
    "        # Use the predicted standard deviations\n",
    "        std_dev = y_std\n",
    "    \n",
    "    # Calculate average log likelihood assuming Gaussian noise\n",
    "    log_likelihood = np.mean(stats.norm.logpdf(y, loc=y_mean, scale=std_dev))\n",
    "    \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store GP results\n",
    "gp_results = {}\n",
    "\n",
    "for group in x_train_data.keys():\n",
    "    first_level, second_level = group\n",
    "    X_train = x_train_data[group]\n",
    "    y_train = y_train_data[group]\n",
    "    X_dev = x_dev_data[group]\n",
    "    y_dev = y_dev_data[group]\n",
    "    \n",
    "    # Calculate residuals for y_train and y_dev using the Lasso model\n",
    "    lasso_model = lasso_results[group]['model']\n",
    "    y_train = y_train - lasso_model.predict(X_train)\n",
    "    y_dev = y_dev - lasso_model.predict(X_dev)\n",
    "    \n",
    "    print(f\"Training GP for {first_level}-{second_level}...\")\n",
    "    \n",
    "    # Define kernel: signal variance × RBF + noise term\n",
    "    kernel = (\n",
    "        C(1.0, (1e-3, 1e3))   # signal variance\n",
    "        * RBF(1.0, (1e-2, 1e2))  # length-scale\n",
    "        # + DotProduct()      # global linear trend\n",
    "        + WhiteKernel(\n",
    "            noise_level=1e-2,\n",
    "            noise_level_bounds=(1e-5, 1e1)\n",
    "          )\n",
    "    )\n",
    "    \n",
    "    # Instantiate GPR\n",
    "    gpr = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        n_restarts_optimizer=10,\n",
    "        random_state=0,\n",
    "        normalize_y=False  # since we've already normalized\n",
    "    )\n",
    "    \n",
    "    # Fit on the training set\n",
    "    gpr.fit(X_train, y_train)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate on dev set\n",
    "y_dev_pred, y_dev_std = gpr.predict(X_dev, return_std=True)\n",
    "\n",
    "r2_dev = r2_score(y_dev, y_dev_pred)\n",
    "r2_train = gpr.score(X_train, y_train)\n",
    "LL_train = calculate_gpr_likelihood(gpr)\n",
    "LL_dev = calculate_gpr_likelihood(gpr, X_dev, y_dev)\n",
    "rmse_train, nrmse_train = calculate_normalized_rmse(y_train, gpr.predict(X_train))\n",
    "rmse_dev, nrmse_dev = calculate_normalized_rmse(y_dev, y_dev_pred)\n",
    "\n",
    "# Store results\n",
    "gp_results[group] = {\n",
    "    'model': gpr,\n",
    "    'kernel': gpr.kernel_,\n",
    "    'r2_dev': r2_dev,\n",
    "    'r2_train': r2_train,\n",
    "    'LL_dev': LL_dev,\n",
    "    'LL_train': LL_train,\n",
    "    'nrmse_train': rmse_train,\n",
    "    'nrmse_dev': nrmse_dev\n",
    "}\n",
    "\n",
    "print(f\"Train R² = {r2_train:.3f}, Dev R² = {r2_dev:.3f}\")\n",
    "print(f\"Log-Likelihood Train: {LL_train :.3f}, Dev: {LL_dev:.3f}\")\n",
    "print(f\"RMSE Train: {rmse_train:.3f}, Dev: {rmse_dev:.3f}\")\n",
    "print(\"Learned kernel:\", gpr.kernel_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gpr_fit(gpr_model, X, y, hue=None, style=None, feature_names=None, top_features=10):\n",
    "    \"\"\"\n",
    "    Create a comprehensive visualization of GPR model fit with better feature interpretation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gpr_model : GaussianProcessRegressor\n",
    "        The fitted GPR model\n",
    "    X : DataFrame\n",
    "        Input features as pandas DataFrame\n",
    "    y : Series or array\n",
    "        Target values\n",
    "    feature_names : list, optional\n",
    "        Names of features (if not provided, will use X.columns)\n",
    "    top_features : int, optional\n",
    "        Number of top features to show in importance plot\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Convert y to numpy array if it's a pandas Series\n",
    "    if hasattr(y, 'values'):\n",
    "        y_values = y.values\n",
    "    else:\n",
    "        y_values = np.array(y)\n",
    "    \n",
    "    # Get feature names from DataFrame if not provided\n",
    "    if feature_names is None and hasattr(X, 'columns'):\n",
    "        feature_names = X.columns.tolist()\n",
    "    elif feature_names is None:\n",
    "        feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    # Get predictions and uncertainty\n",
    "    y_pred, y_std = gpr_model.predict(X, return_std=True)\n",
    "    if hue is None:\n",
    "        hue = pd.Series(y_std, index=X.index, name='Prediction Uncertainty')\n",
    "    elif hue is not None and hasattr(X, 'index'):\n",
    "        hue = hue.loc[X.index]\n",
    "\n",
    "    if style is not None and hasattr(X, 'index'):\n",
    "        style = style.loc[X.index]\n",
    "    \n",
    "       # Calculate metrics\n",
    "    r2 = gpr_model.score(X, y_values)\n",
    "    rmse = np.sqrt(mean_squared_error(y_values, y_pred))\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(16, 14))\n",
    "    \n",
    "    # 1. Prediction vs Actual plot\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    sns.scatterplot(x=y_values, y=y_pred, hue=hue, style=style, alpha=0.6, ax=ax1)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(min(y_values), min(y_pred))\n",
    "    max_val = max(max(y_values), max(y_pred))\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    ax1.set_xlabel('Actual Values')\n",
    "    ax1.set_ylabel('Predicted Values')\n",
    "    ax1.set_title(f'Prediction vs Actual\\nR² = {r2:.4f}, RMSE = {rmse:.4f}')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Prediction vs Actual with Uncertainty\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    # Sort by actual values for clearer visualization\n",
    "    sort_idx = np.argsort(y_values)\n",
    "    ax2.errorbar(np.arange(len(y_values)), y_values[sort_idx], yerr=0, fmt='o', label='Actual', alpha=0.6)\n",
    "    ax2.errorbar(np.arange(len(y_values)), y_pred[sort_idx], yerr=1.96*y_std[sort_idx], \n",
    "                fmt='o', label='Predicted with 95% CI', alpha=0.6)\n",
    "    ax2.set_xlabel('Sample Index (sorted by actual value)')\n",
    "    ax2.set_ylabel('Value')\n",
    "    ax2.set_title('GPR Predictions with Uncertainty')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residuals plot\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    residuals = y_values - y_pred\n",
    "    sns.scatterplot(x=y_pred, y=residuals, hue=hue, style=style, alpha=0.6, ax=ax3)\n",
    "    ax3.axhline(y=0, color='r', linestyle='--')\n",
    "    ax3.set_xlabel('Predicted Values')\n",
    "    ax3.set_ylabel('Residuals')\n",
    "    ax3.set_title('Residuals vs Predicted')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Uncertainty distribution\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.hist(y_std, bins=20, alpha=0.6)\n",
    "    ax4.set_xlabel('Prediction Standard Deviation')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Distribution of Prediction Uncertainty')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    if hasattr(X, 'values'):\n",
    "        X_values = X.values\n",
    "    else:\n",
    "        X_values = X\n",
    "    \n",
    "    if X_values.shape[1] > 1:\n",
    "        print(\"\\n--- Feature Importance Analysis ---\")\n",
    "        \n",
    "        # Estimate feature importance by varying each feature\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        importance = []\n",
    "        \n",
    "        # Create a grid for each feature\n",
    "        for i in range(X_values.shape[1]):\n",
    "            # Use pandas if available for min/max\n",
    "            if hasattr(X, 'iloc'):\n",
    "                feature_min = X.iloc[:, i].min()\n",
    "                feature_max = X.iloc[:, i].max()\n",
    "            else:\n",
    "                feature_min = np.min(X_values[:, i])\n",
    "                feature_max = np.max(X_values[:, i])\n",
    "                \n",
    "            x_grid = np.linspace(feature_min, feature_max, 50)\n",
    "            X_grid = np.tile(np.mean(X_values, axis=0), (50, 1))\n",
    "            X_grid[:, i] = x_grid\n",
    "            \n",
    "            # Predict across the grid\n",
    "            y_grid = gpr_model.predict(X_grid)\n",
    "            \n",
    "            # Calculate importance as range of predictions\n",
    "            importance.append(np.max(y_grid) - np.min(y_grid))\n",
    "        \n",
    "        # Create DataFrame for importance\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        })\n",
    "        \n",
    "        # Sort by importance\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Show top features\n",
    "        top_importance = importance_df.head(top_features)\n",
    "        \n",
    "        # Plot top feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=top_importance)\n",
    "        plt.title(f'Top {top_features} Feature Importance in GPR Model')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display importance table\n",
    "        print(\"\\nFeature Importance Ranking:\")\n",
    "        display(importance_df)\n",
    "        \n",
    "        # Feature correlation with target\n",
    "        if hasattr(X, 'corrwith'):\n",
    "            print(\"\\nFeature Correlation with Target:\")\n",
    "            corr_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Correlation': X.corrwith(pd.Series(y_values, index=X.index)).values\n",
    "            })\n",
    "            corr_df['Abs_Correlation'] = np.abs(corr_df['Correlation'])\n",
    "            corr_df = corr_df.sort_values('Abs_Correlation', ascending=False)\n",
    "            display(corr_df)\n",
    "            \n",
    "            # Plot correlation heatmap for top features\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            top_features_list = importance_df.head(min(15, len(feature_names)))['Feature'].tolist()\n",
    "            X_top = X[top_features_list]\n",
    "            \n",
    "            # Add target to the correlation matrix\n",
    "            X_with_y = X_top.copy()\n",
    "            X_with_y['Target'] = y_values\n",
    "            \n",
    "            # Plot correlation heatmap\n",
    "            sns.heatmap(X_with_y.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "            plt.title('Correlation Heatmap of Top Features with Target')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# For a specific group:\n",
    "group = (True, True)  # or any other group you're interested in\n",
    "# group = ('all', 'all')  # or any other group you're interested in\n",
    "X_train = x_train_data[group]\n",
    "y_train = y_train_data[group]\n",
    "X_dev = x_dev_data[group]\n",
    "y_dev = y_dev_data[group]\n",
    "y_train = y_train - lasso_results[group]['model'].predict(X_train)\n",
    "y_dev = y_dev - lasso_results[group]['model'].predict(X_dev)\n",
    "model = gp_results[group]['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize with better feature interpretation\n",
    "visualize_gpr_fit(model, X_train, y_train, hue=df['openingType'], style=df['openingType'])\n",
    "\n",
    "# To see only top 5 most important features\n",
    "visualize_gpr_fit(model, X_dev, y_dev, hue=None, style=df['slAll'], top_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7046b5",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e377c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance on dev set across all groups\n",
    "import pandas as pd\n",
    "\n",
    "# Compare model performance on dev set across all groups\n",
    "\n",
    "# Define metrics to extract and their properties\n",
    "metrics = {\n",
    "    'R²': {'key': 'r2_dev', 'higher_is_better': True, 'description': 'R² scores on dev set (higher is better)'},\n",
    "    'Log-Likelihood': {'key': 'LL_dev', 'higher_is_better': True, 'description': 'Log-likelihood scores on dev set (higher is better)'},\n",
    "    'NRMSE': {'key': 'nrmse_dev', 'higher_is_better': False, 'description': 'Normalized RMSE scores on dev set (lower is better)'}\n",
    "}\n",
    "\n",
    "# Model types to compare\n",
    "models = ['Linear', 'Ridge', 'Lasso', 'GP']\n",
    "model_results = {'Linear': lr_results, 'Ridge': ridge_results, 'Lasso': lasso_results, 'GP': gp_results}\n",
    "\n",
    "# Process all metrics\n",
    "results_df = {}\n",
    "for metric_name, metric_info in metrics.items():\n",
    "    metric_key = metric_info['key']\n",
    "    higher_is_better = metric_info['higher_is_better']\n",
    "    \n",
    "    # Extract metrics for all groups and models\n",
    "    metric_data = []\n",
    "    for group in x_train_data.keys():\n",
    "        first_level, second_level = group\n",
    "        \n",
    "        row_data = {'first_level': first_level, 'second_level': second_level}\n",
    "        for model_name, model_result in model_results.items():\n",
    "            row_data[model_name] = model_result[group][metric_key]\n",
    "        \n",
    "        metric_data.append(row_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    metric_df = pd.DataFrame(metric_data)\n",
    "    metric_df = metric_df.set_index(['first_level', 'second_level'])\n",
    "    results_df[metric_name] = metric_df\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{metric_info['description']}:\")\n",
    "    display(metric_df)\n",
    "    print(f\"\\nAverage {metric_name} by model type:\")\n",
    "    print(metric_df.mean())\n",
    "    \n",
    "    # Find best model for each group\n",
    "    if higher_is_better:\n",
    "        best_model = metric_df.idxmax(axis=1)\n",
    "    else:\n",
    "        best_model = metric_df.idxmin(axis=1)\n",
    "    \n",
    "    print(f\"\\nBest model for each group ({metric_name}):\")\n",
    "    display(pd.DataFrame({f'Best Model ({metric_name})': best_model}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324a541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycascade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
